# 主要功能

对[arxiv网站](https://arxiv.org/)的论文检索结果页面，实现元数据抽取的基本需求，支持普通检索和高级检索。

# 爬取页面的思想

该爬取方法首先需要手动获取**检索结果界面**的第一页，然后从**第一页的html**中找到**下一页的url**；接着访问下一页url的html内容，然后找到其中下一页的url，并保存下一页的html内容至`page_html文件夹`。不断重复这个过程，直到没有下一页的链接。最终所有页面的url链接（除了第一页）都保存在`page_link.txt`中，所有页面的html链接都保存在`page_html文件夹`。

# 抽取数据的思想

根据爬取的html内容，从中抽取所需的数据，保存为以下md格式：

```md
### 标题: {title} 
> **arxiv**: {identifier}
> **Authors**: {authors}
> **First submission**: {"%Y-%m-%d"}
> **First announcement**: {"%Y-%m"}
> **领域**: {categories}
> **摘要**: {abstract}
```

# 使用方法

### 1、页面内容爬取和页面url获取


```bash
python get_pagelink.py
```

执行以上代码，获取所有检索的页面链接（保存至`page_link.txt`）和页面内容（保存至`page_html文件夹`），为了保证`page_link.txt`的完整性，在执行完代码后**第一页的链接请手动添加**。

如果检索的结果超过10000条，则无法一次性爬取（因为arxiv最多展示50页结果，每页最多展示200条），需要自行优化检索条件将一次的检索结果限制在10000条内，然后分批次爬取。

在分批次爬取的过程中，每进行新的爬取时，需要更新`1.html`中的内容（其中是手动检索得到的检索结果第一页的信息）。**注意修改`page_link.txt`和`page_html文件夹`的名称**，以免新的爬取内容覆盖已有的内容。**最好还要修改**`page_html文件夹`中的命名方式，方便后续可能存在的合并文件夹操作。

### 2、抽取元数据

```bash
python extract_arxiv.py
```

执行以上代码用以抽取元数据。

# 优势
- 因为一开始需要手动在arxiv页面进行检索，所以能通过GUI灵活自定义检索词，也支持不使用检索词的检索。
- 不会出现因为过于频繁访问而被封ip的情况。

# 不足
- 整个流程没有完全严格实现自动化，但也满足基本的使用需求。
- 没有考虑基于时间的增量更新。
- 没有采用异步网络请求，面对繁重的爬取任务处理时间可能较长。

# 致谢
感谢[arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler)的启发。